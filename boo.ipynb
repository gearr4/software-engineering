{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722136aa-83d4-4ee6-8c6d-b513a1dd9ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a policy defines the learning agent’s way of behaving at a given time roughly speaking a policy is a mapping from perceived states of the environment to actions to be taken when in those states it corresponds to what in psychology would be called a set of stimulus–response rules or associations in some cases the policy may be a simple function or lookup table whereas in others it may involve extensive computation such as a search process the policy is the core of a reinforcement learning agent in the sense that it alone is sucient to determine behavior in general policies may be stochastic specifying probabilities for each action a reward signal defines the goal of a reinforcement learning problem on each time step the environment sends to the reinforcement learning agent a single number called the reward the agent’s sole objective is to maximize the total reward it receives over the long run the reward signal thus defines what are the good and bad events for the agent in a biological system we might think of rewards as analogous to the experiences of pleasure or pain they are the immediate and defining features of the problem faced by the agent the reward signal is the primary basis for altering the policy if an action selected by the policy is followed by low reward then the policy may be changed to select some other action in that situation in the future in general reward signals may be stochastic functions of the state of the environment and the actions taken whereas the reward signal indicates what is good in an immediate sense a value function specifies what is good in the long run roughly speaking the value of a state is the total amount of reward an agent can expect to accumulate over the future starting from that state whereas rewards determine the immediate intrinsic desirability of environmental states values indicate the longterm desirability of states after taking into account the states that are likely to follow and the rewards available in those states for example a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards or the reverse could be true to make a human analogy rewards are somewhat like pleasure if high and pain if low whereas values correspond to a more refined and farsighted judgment of how pleased or displeased we are that our environment is in a particular state rewards are in a sense primary whereas values as predictions of rewards are secondary without rewards there could be no values and the only purpose of estimating values is to achieve more reward nevertheless it is values with which we are most concerned when making and evaluating decisions action choices are made based on value judgments we seek actions that bring about states of highest value not highest reward because these actions obtain the greatest amount of reward for us over the long run unfortunately it is much harder to determine values than it is to determine rewards rewards are basically given directly by the environment but values must be estimated and reestimated from\n"
     ]
    }
   ],
   "source": [
    "#first requirement\n",
    "import string\n",
    "import os\n",
    "\n",
    "file_path = 'boo.txt'\n",
    "\n",
    "  \n",
    "text = open(\"boo.txt\", \"r\")\n",
    "\n",
    "# check if size of file is 0\n",
    "if os.stat(file_path).st_size == 0:\n",
    "    text_file = open(\"boo.txt\", \"w\")\n",
    "    text = text_file.write(input(\"ENTER THE PARAGRAPH:\"))\n",
    "    text_file.close()\n",
    "    print(text)\n",
    "\n",
    "d = dict()\n",
    "print(text.read())\n",
    "text = open(\"boo.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7775999c-684b-49e8-a7b6-ed81836d0e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : 21\n",
      "policy : 7\n",
      "defines : 3\n",
      "the : 43\n",
      "learning : 4\n",
      "agent’s : 2\n",
      "way : 1\n",
      "of : 19\n",
      "behaving : 1\n",
      "at : 1\n",
      "given : 2\n",
      "time : 2\n",
      "roughly : 2\n",
      "speaking : 2\n",
      "is : 15\n",
      "mapping : 1\n",
      "from : 3\n",
      "perceived : 1\n",
      "states : 8\n",
      "environment : 5\n",
      "to : 15\n",
      "actions : 4\n",
      "be : 9\n",
      "taken : 2\n",
      "when : 2\n",
      "in : 15\n",
      "those : 2\n",
      "it : 8\n",
      "corresponds : 1\n",
      "what : 4\n",
      "psychology : 1\n",
      "would : 1\n",
      "called : 2\n",
      "set : 1\n",
      "stimulus–response : 1\n",
      "rules : 1\n",
      "or : 5\n",
      "associations : 1\n",
      "some : 2\n",
      "cases : 1\n",
      "may : 5\n",
      "simple : 1\n",
      "function : 2\n",
      "lookup : 1\n",
      "table : 1\n",
      "whereas : 5\n",
      "others : 1\n",
      "involve : 1\n",
      "extensive : 1\n",
      "computation : 1\n",
      "such : 1\n",
      "as : 3\n",
      "search : 1\n",
      "process : 1\n",
      "core : 1\n",
      "reinforcement : 3\n",
      "agent : 5\n",
      "sense : 3\n",
      "that : 7\n",
      "alone : 1\n",
      "sucient : 1\n",
      "determine : 4\n",
      "behavior : 1\n",
      "general : 2\n",
      "policies : 1\n",
      "stochastic : 2\n",
      "specifying : 1\n",
      "probabilities : 1\n",
      "for : 5\n",
      "each : 2\n",
      "action : 4\n",
      "reward : 13\n",
      "signal : 4\n",
      "goal : 1\n",
      "problem : 2\n",
      "on : 2\n",
      "step : 1\n",
      "sends : 1\n",
      "single : 1\n",
      "number : 1\n",
      "sole : 1\n",
      "objective : 1\n",
      "maximize : 1\n",
      "total : 2\n",
      "receives : 1\n",
      "over : 3\n",
      "long : 3\n",
      "run : 3\n",
      "thus : 1\n",
      "are : 10\n",
      "good : 3\n",
      "and : 9\n",
      "bad : 1\n",
      "events : 1\n",
      "biological : 1\n",
      "system : 1\n",
      "we : 4\n",
      "might : 2\n",
      "think : 1\n",
      "rewards : 10\n",
      "analogous : 1\n",
      "experiences : 1\n",
      "pleasure : 2\n",
      "pain : 2\n",
      "they : 1\n",
      "immediate : 4\n",
      "defining : 1\n",
      "features : 1\n",
      "faced : 1\n",
      "by : 5\n",
      "primary : 2\n",
      "basis : 1\n",
      "altering : 1\n",
      "if : 3\n",
      "an : 3\n",
      "selected : 1\n",
      "followed : 2\n",
      "low : 3\n",
      "then : 1\n",
      "changed : 1\n",
      "select : 1\n",
      "other : 2\n",
      "situation : 1\n",
      "future : 2\n",
      "signals : 1\n",
      "functions : 1\n",
      "state : 5\n",
      "indicates : 1\n",
      "value : 5\n",
      "specifies : 1\n",
      "amount : 2\n",
      "can : 1\n",
      "expect : 1\n",
      "accumulate : 1\n",
      "starting : 1\n",
      "intrinsic : 1\n",
      "desirability : 2\n",
      "environmental : 1\n",
      "values : 8\n",
      "indicate : 1\n",
      "longterm : 1\n",
      "after : 1\n",
      "taking : 1\n",
      "into : 1\n",
      "account : 1\n",
      "likely : 1\n",
      "follow : 1\n",
      "available : 1\n",
      "example : 1\n",
      "always : 1\n",
      "yield : 2\n",
      "but : 2\n",
      "still : 1\n",
      "have : 1\n",
      "high : 3\n",
      "because : 2\n",
      "regularly : 1\n",
      "reverse : 1\n",
      "could : 2\n",
      "true : 1\n",
      "make : 1\n",
      "human : 1\n",
      "analogy : 1\n",
      "somewhat : 1\n",
      "like : 1\n",
      "correspond : 1\n",
      "more : 2\n",
      "refined : 1\n",
      "farsighted : 1\n",
      "judgment : 1\n",
      "how : 1\n",
      "pleased : 1\n",
      "displeased : 1\n",
      "our : 1\n",
      "particular : 1\n",
      "predictions : 1\n",
      "secondary : 1\n",
      "without : 1\n",
      "there : 1\n",
      "no : 1\n",
      "only : 1\n",
      "purpose : 1\n",
      "estimating : 1\n",
      "achieve : 1\n",
      "nevertheless : 1\n",
      "with : 1\n",
      "which : 1\n",
      "most : 1\n",
      "concerned : 1\n",
      "making : 1\n",
      "evaluating : 1\n",
      "decisions : 1\n",
      "choices : 1\n",
      "made : 1\n",
      "based : 1\n",
      "judgments : 1\n",
      "seek : 1\n",
      "bring : 1\n",
      "about : 1\n",
      "highest : 2\n",
      "not : 1\n",
      "these : 1\n",
      "obtain : 1\n",
      "greatest : 1\n",
      "us : 1\n",
      "unfortunately : 1\n",
      "much : 1\n",
      "harder : 1\n",
      "than : 1\n",
      "basically : 1\n",
      "directly : 1\n",
      "must : 1\n",
      "estimated : 1\n",
      "reestimated : 1\n"
     ]
    }
   ],
   "source": [
    "for line in text:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    line = line.translate(line.maketrans(\"\", \"\", string.punctuation))\n",
    "    words = line.split(\" \")\n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word] = d[word] + 1\n",
    "        else:\n",
    "            d[word] = 1\n",
    "  \n",
    "\n",
    "for key in list(d.keys()):\n",
    "    print(key, \":\", d[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f6b296-52a0-417b-8d55-88c9cf6b27eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter word that need to be searched: taj\n",
      "enter word to replace sword: a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a policy defines the learning agent’s way of behaving at a given time roughly speaking a policy is a mapping from perceived states of the environment to actions to be taken when in those states it corresponds to what in psychology would be called a set of stimulus–response rules or associations in some cases the policy may be a simple function or lookup table whereas in others it may involve extensive computation such as a search process the policy is the core of a reinforcement learning agent in the sense that it alone is sucient to determine behavior in general policies may be stochastic specifying probabilities for each action a reward signal defines the goal of a reinforcement learning problem on each time step the environment sends to the reinforcement learning agent a single number called the reward the agent’s sole objective is to maximize the total reward it receives over the long run the reward signal thus defines what are the good and bad events for the agent in a biological system we might think of rewards as analogous to the experiences of pleasure or pain they are the immediate and defining features of the problem faced by the agent the reward signal is the primary basis for altering the policy if an action selected by the policy is followed by low reward then the policy may be changed to select some other action in that situation in the future in general reward signals may be stochastic functions of the state of the environment and the actions taken whereas the reward signal indicates what is good in an immediate sense a value function specifies what is good in the long run roughly speaking the value of a state is the total amount of reward an agent can expect to accumulate over the future starting from that state whereas rewards determine the immediate intrinsic desirability of environmental states values indicate the longterm desirability of states after taking into account the states that are likely to follow and the rewards available in those states for example a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards or the reverse could be true to make a human analogy rewards are somewhat like pleasure if high and pain if low whereas values correspond to a more refined and farsighted judgment of how pleased or displeased we are that our environment is in a particular state rewards are in a sense primary whereas values as predictions of rewards are secondary without rewards there could be no values and the only purpose of estimating values is to achieve more reward nevertheless it is values with which we are most concerned when making and evaluating decisions action choices are made based on value judgments we seek actions that bring about states of highest value not highest reward because these actions obtain the greatest amount of reward for us over the long run unfortunately it is much harder to determine values than it is to determine rewards rewards are basically given directly by the environment but values must be estimated and reestimated from\n"
     ]
    }
   ],
   "source": [
    "#2nd requirement\n",
    "text = open(\"boo.txt\", \"r\")\n",
    "sword=input(\"enter word that need to be searched:\")\n",
    "rword=input(\"enter word to replace sword:\")\n",
    "for line in text:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    line = line.translate(line.maketrans(\"\", \"\", string.punctuation))\n",
    "    words = line.split(\" \")\n",
    "    l2 =[]\n",
    "    for item in words:\n",
    "        if item==sword:\n",
    "            l2.append(item.replace(sword,rword))\n",
    "        else:l2.append(item)\n",
    "text=\" \".join(l2)\n",
    "f = open(\"boo.txt\", \"w\")\n",
    "f.write(text)\n",
    "f = open(\"boo.txt\", \"r\")\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787243b6-251a-4475-8af6-d6a1ca11f0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
